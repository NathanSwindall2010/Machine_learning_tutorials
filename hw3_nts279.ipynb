{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 373 Machine Learning Toolbox for Text Analysis, Spring 2019\n",
    "\n",
    "# Homework 3 - due Thursday Mar 12 2020 at 2:00pm\n",
    "\n",
    "For this homework you will hand in (upload) to canvas:\n",
    "- JUST this notebook renamed ``hw3_YourEID.ipynb``\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) an ensure your code outputs the correct answer. I\n",
    "\n",
    "A perfect solution for this homework is worth **100** points. For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible for partial credit. However, make sure it does not have any output errors.Â **If there are any output errors, half of the points for that problem will be automatically deducted.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students and work towards solutions together.  However, all of the code you write must be your own! There is a thread on Piazza where you can look for a study group: https://piazza.com/class/k5o768x0bq6ai?cid=10\n",
    "\n",
    "Review extension and academic dishonesty policy here: https://jjessyli.github.io/courses/lin373#extension-policy\n",
    "\n",
    "For typing up your answers to non-programming problems, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please list any collaborators here:\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Logistic Regression\n",
    "\n",
    "* **(a)** (5 points) In logistic regression, we know that when there are two classes an example can belong to ($y\\in\\{0,1\\}$), for each example ${\\bf x}$ with M features,\n",
    "    $$\n",
    "    p(y=1|x_1, x_2, ..., x_M) = \\frac{1}{1+\\exp{ (-\\sum_{j=1}^M w_jx_j-b) }}\n",
    "    $$\n",
    "    $$\n",
    "    p(y=0|x_1, x_2, ..., x_M) = 1-p(y=1|x_1, x_2, ..., x_M)\n",
    "    $$\n",
    "Generally, how many parameters does logistic regression estimate? Please explain any variables in your answer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32e8d01c1181802cdc97a61b8a6d7c3a",
     "grade": true,
     "grade_id": "answer-1a",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "The logistic regression usually has many different parameters. It is dependent on how features there are. For example, usually for every feature, you have an associated weight, and then you also have a bias term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b)** (10 points) Given a dataset of $N$ examples, briefly describe how these parameters are estimated. You do not need to lay out specific mathematical derivations, rather, provide the name(s) of the process(es) and give a few sentences describing the process. You can assume that all these examples will be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22fe9da86d4d4ab54966d97048255437",
     "grade": true,
     "grade_id": "answer-1b",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "Usually, you will start off with certain weights for each feature. Say we have four different features, and a bias term. We could set them all to zero. Then, we can multiple the amount of features of a specific data set by these weigths and then put it through a sigmoid function to determine the probability of it being in a certain class. For example, say after multiplying the vectors,adding the bias, and putting it through a sigmoid function we get a number like 0.51. This means that are model has predicted that the data set is of class 1. Say our model is wrong though, and it is actually in class 0. Then we can use the cross-entropy lost function to determine how far we are off. What we want to do is minimize the value we get for the cross-entropy function. To minimize it we will use gradient descent to get the optimal weigths for each feature that minimize the cross entropy function. This will take many iterations as each interation nudges the weights closer to minimizing the cors entropy function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(c)** (10 points) Logistic regression can be extended to classify $K$ classes instead of only 2. When $y$ can be one of K classes, the conditional probability of $y$ being of a particular class $k$ is:\n",
    " $$\n",
    " p(y=k|x_1,x_2,...,x_M) = \\frac{\\exp(\\sum_{j=1}^M w_{jk}x_j+b_k)}{\\sum_{k'=1}^K\\exp(\\sum_{j=1}^M w_{jk'}x_j+b_{k'})}\n",
    " $$\n",
    "How many parameters are we estimating in this case?  Please explain any variables in your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7923625917af8dccbdea63adadceac05",
     "grade": true,
     "grade_id": "answer-1c",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "The multinomial logistic regression is similar to the binary logistic regression. Some exceptions are that we need separate weight vectors for each of the k classes we have, and that we use a softmax function instead of the sigmoid which effect the loss function and gradient function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Regularization and evaluation\n",
    "\n",
    "Recall that with Ridge Regression, we add a _regularization_ term to the log likelihood:\n",
    "$$\n",
    "    l({\\bf w}) = \\log \\prod_{i=1}^N{ p(y^{(i)}|{\\bf x^{(i)},w})}-\\lambda||{\\bf w}||_2^2\n",
    "$$\n",
    "\n",
    "* **(a)** (10 points) What is the goal of this regularization term? Describe how it works in one sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c8b5dea4b1866fe874bf208dc59da48",
     "grade": true,
     "grade_id": "answer-2a",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "The goal is to penalize weights that are really big which could cause problems. This is done by adding a bias term to the equation which makes it less sensitive to changes in it parameters. T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b)** (15 points) Your friend David uses cross validation to pick $\\lambda$ for his application of logistic regression as follows. He sets aside 20\\% of his data as a test set, and then runs cross validation on his training data to get cross validation accuracy with multiple values of $\\lambda$. He also trains a model on the entire training data for each value of $\\lambda$ and computes test accuracy on his held out data. He gets the following results:\n",
    "\n",
    "| $\\lambda$ | Cross-validation accuracy | Test accuracy |\n",
    "|:---------:|:-------------------------:|:-------------:|\n",
    "|     0     |            0.7            |      0.65     |\n",
    "|     1     |            0.75           |      0.7      |\n",
    "|     10    |            0.8            |      0.63     |\n",
    "|    100    |            0.7            |      0.6      |\n",
    "\n",
    "* **(b-i)** What is the optimal $\\lambda$ in terms of test accuracy? Use the variable `optimal_test_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25e3980227e4918cfea5c286bd59d3ac",
     "grade": false,
     "grade_id": "cell-20d754030dda7fb6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize the variable optimal_test_accuracy with the correct answer!\n",
    "# YOUR CODE HERE\n",
    "optimal_test_accuracy = 1\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e034b185c340d58157bd27ab8db9ce2c",
     "grade": true,
     "grade_id": "cell-a549a195010745fe",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Do not edit this cell - hidden autograder test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b-ii)** Is this the same $\\lambda$ that would be chosen by cross validation? Use the variable `optimal_cross_validation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a3fac9e5ac3cead7f053b4c3d9bbe7b",
     "grade": false,
     "grade_id": "cell-53232bb51fea7452",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize the variable optimal_cross_validation with the correct answer!\n",
    "\n",
    "optimal_cross_validation = 10\n",
    "#raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88de11f38bc78088dd9478c097e0ce80",
     "grade": true,
     "grade_id": "cell-3f810ebb88eb1947",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Do not edit this cell - hidden autograder test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b-iii)** David wants to inform the public about his model and his results. What should he report as the test accuracy of his method (logistic regression with regularization parameter $\\lambda$) on this dataset? Use the variable `reported_accuracy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "630713e9f7b802112f88a5405c4fe96b",
     "grade": false,
     "grade_id": "cell-f31fd5f701eca413",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize the variable reported_accuracy with the correct answer!\n",
    "reported_accuracy = 0.63\n",
    "#raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43eb800d1752f26f4954d97f62bfe885",
     "grade": true,
     "grade_id": "cell-17e26c3376faee4b",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Do not edit this cell - hidden autograder test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Logistic Regression in sklearn\n",
    "\n",
    "Logistic regression, it turns out, is a natural \"cousin\" to Naive Bayes; specifically it is the \"discriminative\" variant. First, let's review the code for the sklearn version of [Naive Bayes (sentiment analysis notebook)](https://utexas.instructure.com/courses/1267031/files/folder/Code-Data/10-oop-nb). Note that we define a random state for , so that the answers will be consistent across everyone (it should be 0.8175)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_model accuracy is :  0.8175\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       197\n",
      "           1       0.82      0.82      0.82       203\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# load data using sklearn.datasets.load_files\n",
    "dataset = load_files(\"movie-reviews/\")\n",
    "\n",
    "\n",
    "# split the data into train and test\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "dataset.data, dataset.target, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# vectorize the training data\n",
    "vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "X_train = vectorizer.fit_transform(docs_train)\n",
    "\n",
    "# fit the model with the training data\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# vectorize the test data and predict \n",
    "X_test = vectorizer.transform(docs_test)\n",
    "y_hat_nb = nb_model.predict(X_test)\n",
    "\n",
    "# get accuracy score\n",
    "accuracy_score_nb = metrics.accuracy_score(y_test, y_hat_nb)\n",
    "\n",
    "# print out some data\n",
    "print(\"nb_model accuracy is : \", accuracy_score_nb)\n",
    "print(\"classification report:\\n\", metrics.classification_report(y_test, y_hat_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "assert(accuracy_score_nb == 0.8175)\n",
    "## If this throws and error, pleast contact the TA immediately!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** (10 points) Using the code for NaiveBayes, implement [logistic regression model provided by sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). How do the two models compare in terms of predictive performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab439e430389df722148cf7a42ddfe87",
     "grade": false,
     "grade_id": "cell-6fdc13b1e29e1fad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Natha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_model accuracy is :  0.8475\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       197\n",
      "           1       0.83      0.88      0.85       203\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.85      0.85      0.85       400\n",
      "weighted avg       0.85      0.85      0.85       400\n",
      "\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# load data using sklearn.datasets.load_files\n",
    "dataset = load_files(\"movie-reviews/\")\n",
    "\n",
    "# split the data into train and test\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "dataset.data, dataset.target, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# vectorize the training data\n",
    "vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "X_train = vectorizer.fit_transform(docs_train)\n",
    "\n",
    "\n",
    "# fit the model with the training data\n",
    "logmodel = LogisticRegression() #solver = 'lbfgs'\n",
    "logmodel.fit(X_train, y_train)\n",
    "\n",
    "# vectorize the test data and predict \n",
    "X_test = vectorizer.transform(docs_test)\n",
    "y_hat_lr = logmodel.predict(X_test)\n",
    "\n",
    "# get accuracy score\n",
    "accuracy_score_lr = metrics.accuracy_score(y_test, y_hat_lr)\n",
    "\n",
    "#raise NotImplementedError()\n",
    "\n",
    "print(\"lr_model accuracy is : \", metrics.accuracy_score(y_test, y_hat_lr))\n",
    "print(\"classification report:\\n\", metrics.classification_report(y_test, y_hat_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02c5b3ab434a338a8ced45b561ee494e",
     "grade": true,
     "grade_id": "cell-c18b15a3268013bb",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visable test cases passed!\n"
     ]
    }
   ],
   "source": [
    "## Do not edit this cell - autograder test\n",
    "assert(accuracy_score_lr == 0.8475)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8733a06b427cb95adef38601a8ec84d7",
     "grade": false,
     "grade_id": "cell-1225245cd0051265",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Which model is better?\n",
    "# initialize the variable better_basic_model with either \"nb\" or \"lr\"\n",
    "better_basic_model=\"lr\"\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d326233da9c9bacb7aee8449207de02",
     "grade": true,
     "grade_id": "cell-5b0571001518b504",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Do not edit this cell - hidden autograder test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** (10 points) Instead of performing just one train/test split, try performing cross validation. [See here for documentation on doing this](https://scikit-learn.org/stable/modules/cross_validation.html). Now again compare NB and LR using 10-fold cross-validation. Use random_state = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d65f752493c02751c3bb44cd62b6bc80",
     "grade": false,
     "grade_id": "cell-b23dc25a2ffb21eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Natha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression: 0.823 (+/- 0.06)\n",
      "Accuracy for Naive Bayes: 0.80 (+/- 0.045)\n",
      "Wall time: 5.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# use random_state = 3\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "XX_train = vectorizer.fit_transform(dataset.data)\n",
    "yy_train = dataset.target\n",
    "\n",
    "logmodel = LogisticRegression(random_state=3)\n",
    "nb_xval_scores = cross_val_score(nb_model, X_train, y_train, cv=10)\n",
    "lr_xval_scores = cross_val_score(logmodel, X_train, y_train, cv=10)\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=3)\n",
    "\n",
    "x = np.array(dataset.data)\n",
    "y = np.array(dataset.target)\n",
    "\n",
    "# models\n",
    "logmodel = LogisticRegression()\n",
    "\n",
    "### xx = np.array([1,2,3,4,5])\n",
    "### print(xx[[2,3]]) ##3pretty cool feature for numpy\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "for train, test in kf.split(dataset.data):\n",
    "    print(test)\n",
    "    docs_train, docs_test, y_train,y_test = x[train], x[test],y[train],y[test]\n",
    "    vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "    X_train = vectorizer.fit_transform(docs_train)\n",
    "    logmodel.fit(X_train, y_train)\n",
    "    X_test = vectorizer.transform(docs_test)\n",
    "    y_hat_lr = logmodel.predict(X_test)\n",
    "    accuracy_score_lr = metrics.accuracy_score(y_test, y_hat_lr)\n",
    "    nb_xval_scores.append(accuracy_score_lr)\n",
    "    lr_xval_score = np.append(lr_xval_scores, [accuracy_score_lr])\n",
    "    \n",
    "print(lr_xval_score.mean())   \n",
    "'''\n",
    "\n",
    "print(\"Accuracy for Logistic Regression: %0.3f (+/- %0.2f)\" % (lr_xval_scores.mean(), lr_xval_scores.std() * 2))\n",
    "print(\"Accuracy for Naive Bayes: %0.2f (+/- %0.3f)\" % (nb_xval_scores.mean(), nb_xval_scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b5fb8f8232d0d15ce19366396ab3b5b",
     "grade": true,
     "grade_id": "cell-bfcd9ca91b0ae335",
     "locked": true,
     "points": 9,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_xval_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9b075935edbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Do not edit this cell - autograder test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_xval_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.847\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_xval_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.81\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"visable test cases passed!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr_xval_scores' is not defined"
     ]
    }
   ],
   "source": [
    "## Do not edit this cell - autograder test\n",
    "assert(round(lr_xval_scores.mean(),3) == 0.847)\n",
    "assert(round(nb_xval_scores.mean(), 3) == 0.81)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f540ad8dc1bc05c021c2ec1ac9503a8a",
     "grade": false,
     "grade_id": "cell-17788de9ee0fdc4d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "print(nb_xval_scores)\n",
    "my_array= np.array(nb_xval_scores)\n",
    "print(my_array.mean())\n",
    "print(sum(nb_xval_scores)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76040757b0d394a1427f91fffba3a198",
     "grade": true,
     "grade_id": "cell-7facf3a5cd38d763",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Do not edit this cell - hidden autograder test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** (5 points) Now, let's modify the featurres in our classifier! The first thing we're trying is to have our classifier not only include unigrams, but also bigrams. Modify your `CountVectorizer` to include bigrams and use logistic regression as your classifier. Did the results on 10-fold cross validation improve? (Make sure to use the same folds as before, so the results are comparable!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba7d7cb657347c4f8c16a2759e90e3d9",
     "grade": false,
     "grade_id": "cell-81286f74a36745a6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# include unigrams and bigrams\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = \"english\", ngram_range = (1,2))\n",
    "XX_train = vectorizer.fit_transform(dataset.data)\n",
    "yy_train = dataset.target\n",
    "\n",
    "logmodel = LogisticRegression(random_state=3)\n",
    "nb_binary_scores = cross_val_score(nb_model, XX_train, yy_train, cv=10)\n",
    "lr_binary_scores = cross_val_score(logmodel, XX_train, yy_train, cv=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy for Logistic Regression: %0.3f (+/- %0.2f)\" % (lr_binary_scores.mean(), lr_binary_scores.std() * 2))\n",
    "print(\"Accuracy for Naive Bayes: %0.2f (+/- %0.3f)\" % (nb_binary_scores.mean(), nb_binary_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8c63b0525ee81894c93acc170da5c06",
     "grade": true,
     "grade_id": "cell-6a24d3fda9d1c9d3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Do not edit this cell - autograder test\n",
    "assert(round(lr_binary_scores.mean(),3) == 0.865)\n",
    "assert(round(nb_binary_scores.mean(), 3) == 0.83)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** (25 points) Let's also add some features. Specifically, we will add the count of part-of-speech (POS): for a particular document, how many nouns are there? How many verbs?... and so on.\n",
    "\n",
    "* **(d-i)** write a function `get_pos_tags(text)` that when given a document, returns POS tag counter. Use NLTK for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "959eaa53a7107507259510c2240762ea",
     "grade": false,
     "grade_id": "cell-bebd5ef221f28c34",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "def get_pos_tags(text) -> Counter:\n",
    "    \"\"\" when given a string, returns a POS tag counter, using NLTK\"\"\"\n",
    "    text = str(text)\n",
    "    # YOUR CODE HERE\n",
    "    sent_tokens = nltk.sent_tokenize(text)\n",
    "    sent_tokens = [nltk.word_tokenize(sentence) for sentence in sent_tokens]\n",
    "    pos_sentences = nltk.pos_tag_sents(sent_tokens);\n",
    "    \n",
    "    \n",
    "    concatenated = []\n",
    "    for sentence in pos_sentences:\n",
    "        concatenated = concatenated + sentence\n",
    "    \n",
    "    return Counter([item[1] for item in concatenated])\n",
    "\n",
    "\n",
    "counter = get_pos_tags(\"This is a test case with a two singular nouns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "144aee0964b4c28aec58ca2a88af4d36",
     "grade": true,
     "grade_id": "cell-dffa39a8d4b55b08",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Do not edit this cell - autograder test\n",
    "assert(get_pos_tags(\"This is a test case with a two singular nouns.\")[\"NN\"] == 2)\n",
    "assert(get_pos_tags(\"This is a test case with multiple sentences. Will you get it right?\")[\"MD\"] == 1)\n",
    "assert(get_pos_tags(str(docs_train[0]))[\"JJ\"] == 98)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(d-ii)** now, write a function `get_pos_features` to create a dataframe for the POS features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2768d36e53c094fb1c3451bf82037ba",
     "grade": false,
     "grade_id": "cell-0da2866a5e6d384b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "def get_pos_features(docs: list) -> pd.DataFrame:\n",
    "    \n",
    "    pos_docs = [get_pos_tags(x) for x in docs]\n",
    "    rows = [x for x in range(len(docs))]\n",
    "    panda = pd.DataFrame(pos_docs,rows)\n",
    "    \n",
    "    return panda.fillna(0)\n",
    "\n",
    "print(get_pos_features(docs_train[:2])[\"NNP\"])\n",
    "get_pos_features(docs_train[:2]).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7e84e342c73f50004201eebce9cbf80",
     "grade": true,
     "grade_id": "cell-350dd0be2e32678d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Do not edit this cell - autograder test\n",
    "assert(get_pos_features(docs_train[:2])[\"JJ\"][0] == 98)\n",
    "# make sure you take care of any nans and make them 0! \n",
    "# this will be necessary for later!\n",
    "assert(get_pos_features(docs_train[:2])[\"NNP\"][1] == 0) \n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle \n",
    "\n",
    "# # run get_pos_features on all the data\n",
    "# pos_features = get_pos_features(dataset.data)\n",
    "\n",
    "# # pickle pos_features\n",
    "# with open('pos_features.pickle', 'wb') as f:\n",
    "#     pickle.dump(pos_features, f)\n",
    "\n",
    "#load pickled pos_features\n",
    "with open('pos_features.pickle', 'rb') as f:\n",
    "     pos_features = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(d-ii)** now, write a function `combine_data` and use ColumnTransformer to combine *unigram* and POS features. Note, using ColumnTransformer will return a numpy array with a smaller amount of data, but a scipi sparse matrix with more data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d029900524d3d0e58d0d3bc90f53e569",
     "grade": false,
     "grade_id": "cell-51b60e6c8888218b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import scipy\n",
    "\n",
    "def transform_data(raw_data) -> pd.DataFrame:\n",
    "\n",
    "    # if it's the full dataset, load the pickle!\n",
    "    if len(raw_data) == 1600:\n",
    "        print(\"loading pos from pickle\")\n",
    "        with open('pos_features.pickle', 'rb') as f:\n",
    "             df_pos = pickle.load(f)\n",
    "                \n",
    "    # otherwise get the features for the data\n",
    "    else:  \n",
    "        df_pos = get_pos_features(raw_data)\n",
    "        \n",
    "    ## combine into a dataframe\n",
    "    df_pos.insert(1,\"docs\", raw_data)\n",
    "    ##### column transformer\n",
    "    column_trans = ColumnTransformer(\n",
    "    [(\"words\", CountVectorizer(), \"docs\")],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    X_train = column_trans.fit_transform(df_pos)\n",
    "        \n",
    "    return X_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#stuff = get_unigram_features(docs_train[:3]).toarray()\n",
    "print(transform_data(docs_train[:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccc315ad7ba7dca90e72546e25b84e1b",
     "grade": true,
     "grade_id": "cell-3fc92c39b371c522",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Do not edit this cell - autograder test\n",
    "assert(type(transform_data(docs_train[:3])) == np.ndarray)\n",
    "assert(transform_data(docs_train[:3])[0][5] == 4)\n",
    "assert(transform_data(docs_train[:3])[1][740] == 3)\n",
    "assert(type(transform_data(docs_train[:20])) == scipy.sparse.csr.csr_matrix)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Since it took me 14 minutes to run transform_data  on all the data, \n",
    "# I have pickled my version for you! You will still need to be able to pass the tests on the first three \n",
    "import pickle \n",
    "\n",
    "# # run transform_data on entire dataset\n",
    "# transformed_data = transform_data(dataset.data)\n",
    "\n",
    "# # pickle transformed_data\n",
    "# with open('transformed_data.pickle', 'wb') as f:\n",
    "#     pickle.dump(transformed_data, f)\n",
    "\n",
    "\n",
    "# load pickle\n",
    "with open('transformed_data.pickle', 'rb') as f:\n",
    "     transformed_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(d-iii)** finally, re-train your logistic regression model over the combined features. What is the 10-fold cross validation accuracy? (Make sure to use the same folds as before, so the results are comparable!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "422650cf1cd8ab972928c20749fb4dec",
     "grade": false,
     "grade_id": "cell-04c93ab3bea940b9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#use random_state = 3\n",
    "\n",
    "\n",
    "logmodel = LogisticRegression()\n",
    "lr_xval_pos_scores = cross_val_score(logmodel, transformed_data, yy_train, cv=10)\n",
    "\n",
    "print(\"Accuracy for Naive Bayes: %0.3f (+/- %0.3f)\" % (lr_xval_pos_scores.mean(), lr_xval_pos_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ec55a1b07493c3813a4cc15a784b8c9",
     "grade": true,
     "grade_id": "cell-c34967bf011f9541",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not edit this cell -- autograder\n",
    "assert(round(lr_xval_pos_scores.mean(), 3) == 0.84) \n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(d-iv)** Finally, some analysis: essentially, we have trained two models: LR with unigrams and bigrams, and LR with unigrams and POS tags. This setting allows us to compare the power of the new features (bigrams/POS tags). Which one is the better feature for the sentiment analysis task? Explain why this may be true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b6baa3e7c27dd3444f18ea26f6e7c52",
     "grade": true,
     "grade_id": "cell-fd9caefd6217c3b2",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "It seems that the better model is the one that containes the unigrams and the bigrams. This is because it did better during corss validation than the other model which means that it would probably better generalize the data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
